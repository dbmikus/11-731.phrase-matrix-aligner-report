% English Reference Grammar
% By Dylan B. Mikus
% Spring 2014: 11-731

\documentclass[twocolumn]{article}

\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc}

% Packages for formatting page, paragraph, and line layout/spacing
\usepackage{setspace}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}

\usepackage{enumerate, enumitem}
\usepackage{amsthm, amsmath}
\usepackage{datetime}

% defining variables
\setlength{\columnsep}{0.25in}
\setlength{\parskip}{1mm}
\onehalfspacing
% \doublespacing

% All of our custom commands and environments
\input{equations.tex}
\newcommand{\originalAlign}{\texttt{grow-diag-final-and}}
\newcommand{\phraseAlign}{\texttt{from-phrase-table}}
\newcommand{\phraseIntersectAlign}{\texttt{grow-intersect-phrase}}
\newcommand{\wrapSingleSpacing}[1]{
  \singlespacing
  #1
  \onehalfspacing
}


\title{Alignment Matrix Generation Based on Phrases \\
  11-731: Machine Translation, Spring 2014}
\date{\today}
\author{Dylan Bergeron Mikus}

\begin{document}
% Instructions:
% - Explain your implementation and evaluation.
% - Discuss prior and related work.
% - Analyze your results.
% - Answer the question: what did you learn?
\maketitle{}

\begin{center}
\Large{\textbf{Abstract}}
\end{center}
\begin{quotation}
  \small{
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et accumsan
    purus, eu auctor quam. Pellentesque auctor nibh sem, sed convallis neque
    eleifend nec. Vestibulum vel sapien id nibh pharetra ultrices. Maecenas
    feugiat  magna turpis, eu venenatis nulla ultricies vel. Proin et enim id
    eros rutrum  tempus sit amet a orci. Duis cursus magna in quam fringilla
    cursus. Nulla sed  mauris sit amet metus dignissim viverra in placerat
    nisl. Suspendisse congue  libero ultricies, congue metus eu, iaculis
    orci. Sed sollicitudin tincidunt venenatis. Suspendisse nisi nisi, rhoncus
    in erat at, ultrices pharetra lectus. Vivamus vestibulum, quam feugiat
    pharetra vestibulum, nisi tellus auctor odio, et tempor purus mauris vel
    urna. Duis ut aliquet ante. Fusce non felis lobortis, iaculis sapien a,
    viverra eros. Nam bibendum metus ac velit rhoncus condimentum.
  }
\end{quotation}


\section{Introduction}
In phrase-based translation, we extract phrases based on alignment matrices
generated for each sentence. There are a number of methods for aligning the
words between parallel sentences. For the most part, these all depend on
alignment from hidden Markov models, or IBM models with a few additional
heuristics applied. Research (\cite{wuwang2007}, \cite{dgzk2006}) has shown that
heuristic models applied on top of traditional word-alignments for alignment
matrix generation results in improved phrase translation and higher evaluation
scores.

In state of the art translation systems, phrase-based alignment performs better
than word-based alignment. So, we want to determine if this improvement relation
holds for the alignment matrix generation step. Specifically, we evaluate the
translation quality of a system that uses phrases for the generation of
alignment matrices. After generating a phrase table based on standard alignment
matrix generation and phrase extraction, we go back and reapply the phrase table
to generating new alignment matrices. We call this method \phraseAlign{}.
Then we intersect the \phraseAlign{} matrices with the original alignment
matrices. These are called the \phraseIntersectAlign{} matrices.
Our hope is that \phraseIntersectAlign{} will remove outlying alignment points
that are errors from the word and heuristic based alignment matrix generation.

For our experiments, we use parallel corpus data from the
\href{http://www.statmt.org/wmt13/training-parallel-nc-v8.tgz}
     {\underline{www.statmt.org/wmt13}}
Workshop on Statistical Machine Translation.
Our experiments show that \textbf{TODO what do they show us?}


\section{Background}
\subsection{Phrase-Based Translation}
Phrase-based statistical machine translation is largely the front-running
machine translation method. A standard phrase-based translation system can be
divided into a number of distinct steps:
\wrapSingleSpacing{
\begin{enumerate}
    \item language model training
    \item alignment matrix generation
    \item phrase extraction
    \item translation model training
    \item phrase-based sentence decoding and translation
\end{enumerate}
}
The language model is typically a set of n-gram frequencies (possibly unigram,
bigram, and trigram), summed up across the whole training corpus and then
normalized. Then an IBM model or hidden Markov model for word alignment uses the
language model to align words from source sentences to target sentences and vice
versa. Traditionally, one then applies heuristics on top of these two alignments
to generate a final alignment matrix.
% mathematical model for word alignment translation and stuff
For a given target sentence $e$ of length $m$, source sentence $f$ of length
$n$, and alignment $a$ from $f$ to $e$ for every $e_i \in e$, the probability of
a given translation is:
\[
  p(e | f, m) = \sum_{a \in [0, n]^m} p(a | f,m)
    \times \prod_{i=1}^m p(e_i | f_{a_i})
\]
Then for the max alignment vector $a$ combined with the language model results
according to the alignment, we can use $a$ to make our alignment
matrix. Typically, we will generate alignments from $f$ to $e$ and from $e$ to
$f$ and then apply a heuristic somehow combining these alignments.
For more information on phrase-based MT, Koehn et al \cite{kom2003}'s paper
gives a more substantial description of all the steps involved.

For our project, we use the Moses phrase-based MT system to run the components
of the system. To generate alignment vectors, Moses uses GIZA++. The default
heuristic applied to these vectors is called \originalAlign{}.
Firstly, we use a superscript notation $(k)$ to represent the $k^{\text{th}}$
sentence in the source and target corpus.
For the $k^{\text{th}}$ sentence pair, we define two alignment vectors, one for
each direction:
\[ a^{(k)} = \argmax_a p(f^{(k)}, a\ |\ e^{(k)}, m_k) \]
\[ b^{(k)} = \argmax_b p(e^{(k)}, b\ |\ f^{(k)}, n_k) \]
We have an alignment matrix $A^{(k)}$, where cell $(i,j)$ is $1$ if word $e_i$
and $f_j$ are aligned, and $0$ otherwise.
We define the qualifications for alignment in the following manner:

$g$ says that a matrix cell $(i,j)$ is aligned if it has an aligned
neighbor and if the cell is aligned in either of the alignment vectors
$a^{(k)}$ or $b^{(k)}$.
As such, it is recursively defined based on following functions.
\wrapSmall{\isAlignedFromGrow{}}
%
%
This function just computes all neighboring points (including diagonals) in a
matrix.
\wrapSmall{\neighborsFunc{}}
%
$f$ evaluates to 1 if the given matrix point is in the
intersection of the two alignment vectors.
If the point is not in the intersection, then it evaluates to 1
if the matrix point has some neighbor that is aligned, and that point is also
in the union of the two alignment vectors (i.e., `g` evaluates to 1), then
we return 1, otherwise we return 0.
\wrapSmall{\isAlignedFromInterOrGrow{}}
%
Ultimately, we define each cell in the matrix such that
the cell evaluates to 1 if the it is in the
intersection of both alignment vectors, if it is not in the intersection but is
in the union of the two alignment vectors and neighbors a previously aligned
cell, or finally if the spot was not yet aligned but is in the union of the two
alignment vectors. This has the effect of preferring alignments grown from the
intersection before finally falling back on the union of alignments.
\wrapSmall{\growDiagMatrix{}}
%
%
So, the algorithm starts with the intersection of the alignment vectors, and
then grows out to neighbors that are aligned in the union of the two vectors,
$a^{(k)}$ and $b^{(k)}$.

%% \subsection{Prior Work}

%% \subsection{Related Work}

\subsection{Evaluation Methods}
We record a number of different values to use during our evaluation.
Let $S_0$ be the phrase table generated by \originalAlign{}.
Let $S_1$ be the phrase table generated by \phraseIntersectAlign{}
These include:
\wrapSingleSpacing{
\begin{enumerate}
  \item the number of alignment points removed from
    the \originalAlign{} alignment matrix
    by intersection with
    the \phraseAlign{} alignment matrix.
  \item $s_0 = |S_0|$
  \item $s_1 = |S_1|$
  \item $s_2 = |S_0 \cup S_1|$
  \item $s_3 = |S_0 \cap S_1|$
\end{enumerate}
}

To evaluate the results, we use a number of different metrics, we ultimately
concern ourselves with the difference between the BLEU score from using Moses
phrase-based translation with alignment matrices generated from \originalAlign{}
and the BLEU score from using Moses phrase-based translation with alignment
matrices generated from \phraseIntersectAlign{}.
To dig a little deeper and gain empirical data that can help us understand our
results, we will also look at the differences in the phrase tables and alignment
matrices. It is our hypothesis that \phraseIntersectAlign{} will remove outlying
points that never happen to fall into the most common or probable phrases. This
may have a positive effect on the overall BLEU score, or it might have a
negligible effect on the BLEU score, but it should at least still decrease the
size of our phrase table without much harm to future decoding. This has the
potential to speed up the decoding process for phrase-based translation.

For our development and test set, we use unique corpora within the Workshop on
Statistical Machine Translation 20012 dataset, available at
\href{http://www.statmt.org/wmt12/dev.tgz}
     {\underline{www.statmt.org/wmt12}}.


\section{Implementation}
\subsection{Generating New Alignment Matrices}


\subsection{Method of Evaluation}


\section{Experiments}


\section{Conclusion}


\section*{References}
\begin{thebibliography}{99}
  \bibitem{wuwang2007}
    Wu, Hua and Wang, Haifeng
    (2007).
    Comparative Study of Word Alignment Heuristics and Phrase-Based SMT.
    In \emph{Toshiba (China) Research and
      Development Center}.

  \bibitem{dgzk2006}
    DeNero, John and Gillick, Dan and Zhang, James and Klein, Dan
    (2006).
    Why Generative Phrase Models Underperform Surface Heuristics.
    In \emph{Proceedings of the Workshop on Statistical Machine Translation}.

  \bibitem{kom2003}
    Koehn, Philipp and Och, Franz Josef and Marcu, Daniel
    (2003).
    Statistical Phrase-Based Translation.
    In \emph{Proceedings of HLT-NAACL}.

  \bibitem{mt11731}
    Dyer, Chris and Lavie, Alon
    (2014).
    11-731: Machine Translation.
    \href{http://demo.clab.cs.cmu.edu/sp2014-11731/}
     {\underline{http://demo.clab.cs.cmu.edu/sp2014-11731/}}.
    In Carnegie Mellon University curriculum.

  \bibitem{collins2013}
    Collins, Michael
    (2013).
    Phrase-Based Translation Models.
    From Columbia University CS Department.
\end{thebibliography}
\end{document}

% Local Variables:
% mode: latex
% fill-column: 80
% eval: (auto-fill-mode 1)
% End:
